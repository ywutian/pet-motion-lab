#!/usr/bin/env python3
"""
Flux + IP-Adapter + ControlNet å›¾åƒç”Ÿæˆç®¡é“
"""

import torch
from PIL import Image
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
from config import MODEL_PATHS, GENERATION_CONFIG, DEVICE, PROMPT_TEMPLATE

# å¯¼å…¥ IP-Adapter çš„è‡ªå®šä¹‰æ¨¡å—
ip_adapter_path = Path(__file__).parent.parent / "models" / "ip_adapter" / "flux"
sys.path.insert(0, str(ip_adapter_path))

from pipeline_flux_ipa import FluxPipeline
from transformer_flux import FluxTransformer2DModel
from attention_processor import IPAFluxAttnProcessor2_0
from transformers import AutoProcessor, SiglipVisionModel
from diffusers.utils import load_image


class MLPProjModel(torch.nn.Module):
    """IP-Adapter çš„æŠ•å½±æ¨¡å‹"""
    def __init__(self, cross_attention_dim=768, id_embeddings_dim=512, num_tokens=4):
        super().__init__()
        
        self.cross_attention_dim = cross_attention_dim
        self.num_tokens = num_tokens
        
        self.proj = torch.nn.Sequential(
            torch.nn.Linear(id_embeddings_dim, id_embeddings_dim*2),
            torch.nn.GELU(),
            torch.nn.Linear(id_embeddings_dim*2, cross_attention_dim*num_tokens),
        )
        self.norm = torch.nn.LayerNorm(cross_attention_dim)
        
    def forward(self, id_embeds):
        x = self.proj(id_embeds)
        x = x.reshape(-1, self.num_tokens, self.cross_attention_dim)
        x = self.norm(x)
        return x


class PetImageGenerator:
    """å® ç‰©å›¾åƒç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.device = DEVICE
        self.pipe = None
        self.image_encoder = None
        self.clip_image_processor = None
        self.image_proj_model = None
        self.num_tokens = 128
        
        print(f"ğŸ”§ åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_models(self):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
        
        try:
            # 1. åŠ è½½ Flux Transformer (è‡ªå®šä¹‰ç‰ˆæœ¬æ”¯æŒ IP-Adapter)
            print("  - åŠ è½½ Flux Transformer...")
            transformer = FluxTransformer2DModel.from_pretrained(
                MODEL_PATHS["flux_base"],
                subfolder="transformer",
                torch_dtype=torch.bfloat16
            )
            
            # 2. åŠ è½½ Flux Pipeline (è‡ªå®šä¹‰ç‰ˆæœ¬)
            print("  - åŠ è½½ Flux Pipeline...")
            self.pipe = FluxPipeline.from_pretrained(
                MODEL_PATHS["flux_base"],
                transformer=transformer,
                torch_dtype=torch.bfloat16
            )
            
            # 3. åŠ è½½ Image Encoder (SigLIP)
            print("  - åŠ è½½ Image Encoder (SigLIP)...")
            image_encoder_path = "google/siglip-so400m-patch14-384"
            self.image_encoder = SiglipVisionModel.from_pretrained(
                image_encoder_path,
                torch_dtype=torch.bfloat16
            ).to(self.device)
            self.clip_image_processor = AutoProcessor.from_pretrained(image_encoder_path)
            
            # 4. åŠ è½½ IP-Adapter æŠ•å½±æ¨¡å‹
            print("  - åŠ è½½ IP-Adapter æŠ•å½±æ¨¡å‹...")
            self.image_proj_model = MLPProjModel(
                cross_attention_dim=self.pipe.transformer.config.joint_attention_dim,
                id_embeddings_dim=1152,
                num_tokens=self.num_tokens,
            ).to(self.device, dtype=torch.bfloat16)
            
            # 5. è®¾ç½® IP-Adapter attention processors
            print("  - è®¾ç½® IP-Adapter attention processors...")
            attn_procs = {}
            for name in self.pipe.transformer.attn_processors.keys():
                if name.startswith("transformer_blocks.") or name.startswith("single_transformer_blocks."):
                    attn_procs[name] = IPAFluxAttnProcessor2_0(
                        hidden_size=self.pipe.transformer.config.num_attention_heads * self.pipe.transformer.config.attention_head_dim,
                        cross_attention_dim=self.pipe.transformer.config.joint_attention_dim,
                        num_tokens=self.num_tokens,
                    ).to(self.device, dtype=torch.bfloat16)
                else:
                    attn_procs[name] = self.pipe.transformer.attn_processors[name]
            
            self.pipe.transformer.set_attn_processor(attn_procs)
            
            # 6. åŠ è½½ IP-Adapter æƒé‡
            print("  - åŠ è½½ IP-Adapter æƒé‡...")
            ip_ckpt = Path(MODEL_PATHS["ip_adapter"])
            state_dict = torch.load(ip_ckpt, map_location="cpu")
            self.image_proj_model.load_state_dict(state_dict["image_proj"], strict=True)
            ip_layers = torch.nn.ModuleList(self.pipe.transformer.attn_processors.values())
            ip_layers.load_state_dict(state_dict["ip_adapter"], strict=False)
            
            # 7. ä¼˜åŒ–è®¾ç½®
            if self.device == "cpu":
                print("  - ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ä½†å†…å­˜å……è¶³ï¼‰...")
                # CPU æ¨¡å¼ä¸‹å¯ç”¨å†…å­˜ä¼˜åŒ–
                self.pipe.enable_attention_slicing()
                self.pipe.enable_vae_slicing()
            elif self.device == "mps":
                print("  - å¯ç”¨ MPS å†…å­˜ä¼˜åŒ–...")
                # ä½¿ç”¨ CPU offloading æ¥å‡å°‘ MPS å†…å­˜å ç”¨
                self.pipe.enable_model_cpu_offload()
                self.pipe.enable_attention_slicing()
                self.pipe.enable_vae_slicing()
                print("  - å·²å¯ç”¨ CPU offloadingï¼Œæ¨¡å‹å°†æŒ‰éœ€åŠ è½½åˆ° MPS")
            else:
                # CUDA è®¾å¤‡
                print(f"  - ç§»åŠ¨æ¨¡å‹åˆ° {self.device}...")
                self.pipe = self.pipe.to(self.device)
                self.pipe.enable_model_cpu_offload()

            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def set_ip_adapter_scale(self, scale):
        """è®¾ç½® IP-Adapter å¼ºåº¦"""
        for attn_processor in self.pipe.transformer.attn_processors.values():
            if isinstance(attn_processor, IPAFluxAttnProcessor2_0):
                attn_processor.scale = scale
    
    @torch.inference_mode()
    def get_image_embeds(self, pil_image):
        """è·å–å›¾åƒ embedding"""
        if isinstance(pil_image, Image.Image):
            pil_image = [pil_image]
        clip_image = self.clip_image_processor(images=pil_image, return_tensors="pt").pixel_values
        clip_image_embeds = self.image_encoder(clip_image.to(self.device, dtype=self.image_encoder.dtype)).pooler_output
        clip_image_embeds = clip_image_embeds.to(dtype=torch.bfloat16)
        image_prompt_embeds = self.image_proj_model(clip_image_embeds)
        return image_prompt_embeds

    def generate_image(
        self,
        reference_image_path: str,
        pose_image_path: str = None,
        prompt: str = None,
        ip_adapter_scale: float = None,
        num_inference_steps: int = None,
        guidance_scale: float = None,
        seed: int = None,
        width: int = None,
        height: int = None,
    ) -> Image.Image:
        """
        ç”Ÿæˆå•å¼ å›¾ç‰‡

        Args:
            reference_image_path: å‚è€ƒå® ç‰©å›¾ç‰‡è·¯å¾„
            pose_image_path: å§¿åŠ¿éª¨æ¶å›¾è·¯å¾„ (å¯é€‰)
            prompt: æç¤ºè¯ (å¯é€‰)
            ip_adapter_scale: IP-Adapter å¼ºåº¦ (0-1)
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦
            seed: éšæœºç§å­
            width: å›¾åƒå®½åº¦
            height: å›¾åƒé«˜åº¦

        Returns:
            ç”Ÿæˆçš„å›¾ç‰‡
        """
        # ä½¿ç”¨é»˜è®¤é…ç½®
        ip_adapter_scale = ip_adapter_scale or GENERATION_CONFIG["ip_adapter_scale"]
        num_inference_steps = num_inference_steps or GENERATION_CONFIG["num_inference_steps"]
        guidance_scale = guidance_scale or GENERATION_CONFIG["guidance_scale"]
        width = width or GENERATION_CONFIG["width"]
        height = height or GENERATION_CONFIG["height"]

        # æ„å»ºæç¤ºè¯
        if prompt is None:
            prompt = PROMPT_TEMPLATE["style"]
        else:
            prompt = f"{prompt}, {PROMPT_TEMPLATE['style']}"

        # åŠ è½½å‚è€ƒå›¾ç‰‡
        reference_image = Image.open(reference_image_path).convert("RGB")

        # è·å–å›¾åƒ embedding
        image_prompt_embeds = self.get_image_embeds(reference_image)

        # è®¾ç½® IP-Adapter å¼ºåº¦
        self.set_ip_adapter_scale(ip_adapter_scale)

        # è®¾ç½®éšæœºç§å­
        if seed is None:
            generator = None
        else:
            generator = torch.Generator(self.device).manual_seed(seed)

        # ç”Ÿæˆå›¾åƒ
        images = self.pipe(
            prompt=prompt,
            image_emb=image_prompt_embeds,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
            width=width,
            height=height,
            generator=generator,
        ).images

        return images[0]

    def generate_all_poses(
        self,
        reference_image_path: str,
        output_dir: str = "output",
        **kwargs
    ) -> dict:
        """
        ç”Ÿæˆæ‰€æœ‰å§¿åŠ¿çš„å›¾åƒ

        Args:
            reference_image_path: å‚è€ƒå® ç‰©å›¾ç‰‡è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºå§¿åŠ¿åç§°ï¼Œå€¼ä¸ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        poses = ["sit", "walk", "rest", "sleep"]
        results = {}

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        for pose in poses:
            print(f"\nğŸ¨ ç”Ÿæˆå§¿åŠ¿: {pose}")

            # å§¿åŠ¿éª¨æ¶è·¯å¾„
            pose_image_path = Path(MODEL_PATHS["pose_library"]) / "dog" / f"{pose}.png"

            if not pose_image_path.exists():
                print(f"  âš ï¸ è·³è¿‡: å§¿åŠ¿éª¨æ¶ä¸å­˜åœ¨")
                continue

            # ç”Ÿæˆå›¾åƒ
            image = self.generate_image(
                reference_image_path=reference_image_path,
                pose_image_path=str(pose_image_path),
                **kwargs
            )

            # ä¿å­˜ç»“æœ
            output_file = output_path / f"{pose}.png"
            image.save(output_file)
            results[pose] = str(output_file)

            print(f"  âœ… å®Œæˆ: {output_file}")

        return results


# å…¨å±€å•ä¾‹
_generator = None

def get_generator() -> PetImageGenerator:
    """è·å–å…¨å±€ç”Ÿæˆå™¨å®ä¾‹"""
    global _generator
    if _generator is None:
        _generator = PetImageGenerator()
        _generator.load_models()
    return _generator

